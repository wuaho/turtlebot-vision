Diario de trabajo para el TFG realizado por Juan Jose Requena Lama


27 de Febrero

Los drivers que creo que son los adecuados (http://wiki.ros.org/depth_image_proc)
drivers para microsoft kinect openni camera (http://wiki.ros.org/openni_camera) y openni_launch (http://wiki.ros.org/openni_launch)
Para unir opencv con ros (http://wiki.ros.org/vision_opencv)

Desconozco si tengo que instalar drivers y ejecutar archivos .launch para que la kinect publique topics  para la nube de puntos

28 de Febrero 

Instalado turtlebot en el portatil, voy a intentar que el portatil simule un turtlebot y desde el servidor se suscriba a
un topic que publique el servicio simulado de mi portatil.
Cambiadas las variables del sistema.
Creado el fichero scriptVariables.sh, la intencion de este script es que te introduzca las variables del sistema que 
necesitamos para nuestro entorno. 
Configurado ya pc y portatil para simular al robot tortuga.

Comprobado que, el roscore se ejecuta en el portatil, es decir, en la tortuga, ya que si no fuese asi me habria dejado 
publicar en un topic sin problema alguno, ejecutando por su cuenta un roscore pero en cambio, me ha aparecido un mensaje 
de que no podia conectarse con ROS MASTER URI. 

1 de Marzo

Ahora en los archivos .bashrc aparecen los exports indicados para conseguir la conectividad entre servidor y robot tortuga.
Habria que conseguir que dichos export utilizasen los hostname, en vez de las IPs.

He mirado en la configuracion que quizas si los demas ordenadores no saben resolver el hostname, puede dar problemas segun 
los ejemplos de esta pagina http://wiki.ros.org/ROS/NetworkSetup . 
Cambiando las variables $ROS_HOSTNAME por IPs ahora ya no dan problemas.

2 de Marzo

El topic que crea una Point Cloud es el /camera/depth/points y los mensajes son del tipo sensor_msgs/PointCloud2 .
Hay que ver si el topic es publicado asi por defecto o es accion del rviz. 

¿REALMENTE NECESITO UNA NUBE DE PUNTOS? ¿QUE BENEFICIOS ME PUEDE DAR PARA LO QUE QUIERO HACER?

Estoy aprendiendo a cómo configurar las dependencias de un paquete correctamente modificando su fichero package.xml
Hay mucha informacion, pero en caso de duda, hacer uso del <depend> y ya esta. Ya que esto solo sirve para que otros puedan 
descargar e instalar nuestro paquete correctamente.
Conclusion: solo necesito modificar package.xml y a no ser que haga un nuevo mensaje o servicio, no tocar el CMakeLists.txt .

Quizas el uso de rostopic hz nos puede hacer ver el momento en el que llega el paquete y asi calcular el tiempo y demas.

Para la proxima sesion, hacer un script sencillito de python como en el que viene en el libro o aun mas sencillo para probar
que las dependencias funcionan correctamente. Tras ello, hacer otro donde se utilice la camara y se vea lo que muestra.

3 de Marzo

Cambiada la variable $ROS_MASTER_URI para las pruebas en servidor

rostopic hz TOPIC para saber como de rapido estan llegando los mensajes de un topic. EL resultado pricipal es mensajes 
por segundo.

http://wiki.ros.org/pcl_ros/Tutorials/CloudToImage te enseña como convertir un pointcloud a una imagen y funciona realmente.
Pero creo que eso no es eficiente, es decir, entonces para que queremos la pointcloud si la vamos a convertir en una imagen...

Asi que, por una parte tengo que hacer un codigo que se suscriba a la pointcloud y de ahi, descubrir como puedo utilizarla 
para la computer vision.

4 de Marzo

Version de OpenCV instalada junto a ROS: 3.3.1

El topic que se publica que nos interesa es el /camera/depth/points , mediante rostopic info TOPIC sabemos que los mensajes
publicados en ese topic son: sensor_msgs/PointCloud2
Este tipo de mensaje esta definido en: http://docs.ros.org/en/api/sensor_msgs/html/msg/PointCloud2.html
Paquete de ros para pcl: http://wiki.ros.org/pcl_ros#Subscribing_to_point_clouds
Informacion general de pcl en ros: http://wiki.ros.org/pcl/Overview


¿Puedo utilizar pcl en python? 
No, no se puede. Al menos mi version de ROS no posee los modulos adecuados para utilizarlo, los modulos existentes
son los que aparecen en /opt/ros/kinectic/lib/python2.7/dist-packages/pcl_ros

Correo enviado el profesor sobre si de verdad tengo que utilizar una pcl ya que no se como se utilizaria.

Si utilizase imagenes, se me ocurre que podria utilizarla para reconocer una cara, al reconocerla, el robot se daria la 
vuelta y retrocedería.

Si quisiese que el robot no se chocase, haria uso del laser scan, asi el robot se moveria automaticamente y si detecta que hay 
un obstaculo cerca, pues se gira y continua.

16 de Marzo

Lo que contiene un mensaje del tipo imagen: https://docs.ros.org/en/melodic/api/sensor_msgs/html/msg/Image.html
Lo que contiene un header dentro del mensaje imagen http://docs.ros.org/en/api/std_msgs/html/msg/Header.html

Explicacion del use_sim_time https://answers.ros.org/question/288672/how-use_sim_time-works/

Tengo que decidir que rama tomar de las que ha comentado el profesor. Ahora mismo estaba mirando como tomar los timestamps
publicados en los mensajes de imagen pero el timestamp es diferente al real.

22 de Marzo

Rosbag y rqt_bag parecen bastante interesantes para saber como ha sido el performance de los topics y de sus mensajes. 
Seguramente necesite saber como utilizarlo.

2 de abril

Tengo que echarle ganas o no voy a conseguir sacar esto. Seguir poco a poco las indicaciones del profesor y hacer algo
que este mal pero que mas o menos haga su cometido.

El tipo time llamado stampd dentro del header contiene 2 integers. Estos indican los segundos y nanosegundos que han pasado
desde el epoch. EL epoch es una fecha y hora desde la que se calcula el tiempo en los ordenadores.
Por ejemplo, el unix epoch es el 1 de enero de 1970 00:00:00 UT. 

El timestamp dentro del paquete de la imagen, indica el tiempo que ha pasado desde que se ha encendido el simulador.
http://wiki.ros.org/rospy/Overview/Time
http://wiki.ros.org/Clock

5 de abril

Si queremos que el gazebo o simulacion del turtlebot se abra sin interfaz grafica, tendremos que modificar el launch file 
que se encuentra en /opt/ros/kinetic/share/turtlebot_gazebo/launch .

http://wiki.ros.org/depth_image_proc para procesar las imagenes

6 de abril

12 de abril

CORREO DEL PROFESOR

Algunos detalles:
1    Pon los cv2.imshow   detrás de la:         system_stamp = rospy.get_rostime()
2    Entiendo que system_stamp.nsecs  son los ns adicionales al system_stamp.secs . Entonces, ¿En línea 24,25   de tu captura pantalla , se desordenan los frames? 2375.5  s  y luego 2375.1 s  ?
3    ¿En línea 9,10  de tu captura pantalla , se pierden 5   frames?
4    Parece q la precisión del timestamp es 5 cs. Compruébalo en alguna web de ROS. Porque además el timestamp en el nodo de llegada es más preciso (no tiene sólo saltos de 0.05 en 0.05, sino de 0.01 en 0.01)
     De todas formas está bien (podemos medir hasta 200 fps), pero esto tuyo habrá que mejorarlo :” En estas pruebas, al utilizar una simulación y obtener los paquetes desde el mismo ordenador, la diferencia de tiempo es minima.” NO es minima, ya que son alrededor de 0.15 s . Esto es algo grande. P ej a 10 GB/s de acceso a RAM, una foto de 1 Mpixel, 3 MB tarda solo   0.3 ms  = 0.0003 s . Todo el tiempo hasta 0.15 es overhead del ROS evidentemente . Y tb de los cv2.imshow   . Ponlos detrás del timestamp , a ver cuanto dura.
-    OK a lo de “problemas de escritura en archivos creo que estarían asi resueltos”. Muy buena idea
5    En cuanto a lo de “una API de medición y quería mostrarle qué tenía en mente” , es darle forma de librería a las funciones que usas, para crear tu propia clase con métodos  para medir tiempo más cómodamente  en cualquier nodo ROS (que contendría llamadas ). Todo lo de
o    system_stamp = rospy.get_rostime()
o            data_list.append((str(img_seq), str(camera_stamp.secs), str(camera_stamp.nsecs), str(system_stamp.secs), str(system_stamp.nsecs), str(img_size)))
o    csvfile.write
o    etc
-    que se convierta en algo como: (te versiono otra clase de medir tiempos, aunque  esta  encapsulación podría mejorarse; tampoco soy experto en ing. Sw) . Fíjate en el añadido para  “//por si se hacen varias mediciones y luego se presenta una media aritmética de las mismas y un tiempo minimo”

class ROS_Timer
{
    private:
        double startTime; //it contains the real timer (that was started when PC was switch on
        double result;    // a partial timing between Start() and Stop()
        double totalTime;  // the addition of several partial timing (result). It is the total time since timer was created or ResetAll
        double minimumTime;
        double overhead;
        int times;
        void ReadSeconds (double *Seconds);
        void PrintSeconds (const char *pc, double *Seconds);
        void PrintTimeSeconds (const char *pc, double *Seconds);
        void RestSeconds (double *c_dif, double *c_final, double *c_init);
        void AddSeconds (double *c_dif, double *c_final, double *c_init);
    public:
        ROS_Timer();
        int  NumberOfMeasures(void); //por si se hacen varias mediciones y luego se presenta una media aritmetica de las mismas y un tiempo minimo
        void Start();
        void Stop();
        void Calibrate();
        void Reset();
        void ResetAll();
        void PrintTime(const char* message);
        void PrintMeanTime(const char* message);
        void PrintMinimumTime(const char* message);
}

A ver que sale cuando ejecutes en dos  PCs  por separado el gazebo y el recolector de frames.

COSAS HECHAS

rosbag me va a servir para grabar simulaciones y pruebas, asi las puedo luego reproducir comodamente en mi casa.
rostopic hz TOPIC para saber la frecuencia de publicacion de paquetes en un topic.
he desactivado la gui en el gazebo ya que es innecesaria PERO AHORA LA FRECUENCIA DE PBULICACION DE PAQUETES DE 
/CAMERA/RGB/image_raw ES MENOR WTFFFFFFFF

1.Hecho
2.No están desordenados, es que en realidad el de la linea 24 no es 2375.5 s sino 2375.05 s, luego vendría el 2375.1s.
3. He añadido un contador que se inicia con el numero de secuencia de la primera imagen que le llega al subscriber
y cada vez que le llega una imagen va aumentando en 1. Así, será más fácil visualizar donde se pierden frames.La diferencia 
entre el ultimo valor recogido por el contador y el ultimo numero de secuencia de imagen recibido deberia de ser el numero de
frames perdidos.

El caso es que después de reiniciar varias veces la simulación y probarlo varias veces, no he tenido más problemas de
pérdidas de frames. Dejaré puesto el contador por si me da  problemas en el futuro. Si tuviese de nuevo problemas,
podría ser por la ausencia de configuracion del queue_size en el subscriber cuando llamo a rospy.Subscriber en el script.

4. Para comprobar si realmente los frames se estaban tomando cada 0.05s he utilizado el comando
 <<rostopic hz /camera/rgb/image_raw>> para ver cuantos paquetes por segundo publica el topic al que se suscribe mi nodo.
  INSERTAR CAPTURA 
 Como se puede ver en la imagen, la frecuencia de publicacion de imagenes en el topic ronda entre los 38~42 hz.
 Además, lo interesante es que índica que la publicación más "rápida" ha sido cada 0.010s y la más "lenta" cada 0.050s
 en vez de siempre cada 0.050s como se podría llegar a intuir por la traza del .csv. 
Al realizar una simulación más larga, he podido comprobar que esa diferencia de 0.010s entre toma de imágenes se puede observar
también. INSERTAR CAPTURA 2.
Además, también se puede observar dos imágenes tomadas justo en el mismo instante, he buscado por qué puede ser pero sin éxito.
Quizás se deba a alguna sincronización necesaria que se realiza.

Ahora bien, en cuanto a la precisión de los timestamps, cierto es que solo llegan hasta las centésimas de segundo.
Creo que esto es por la configuración del archivo .world utilizado en la simulación de gazebo. Si la simulación de gazebo es 
actualizada cada 0.01s, no podrá dar un timestamp más preciso que ese.
INSERTAR CAPTURA 3
El archivo .world que utiliza <<roslaunch turtlebot_gazebo turtlebot_world.launch>> se encuentra en 
/opt/ros/kinetic/share/turtlebot_gazebo/worlds/playground.world
INSERTAR IMAGEN 4
(http://gazebosim.org/tutorials?tut=physics_params&cat=physics) Aquí se explica el significado de los diferentes parámetros de la 
simulación que nos interesa como puede ser max_step_size que por defecto en el archivo está en 0.01s de ahí a que 
el tiempo en nuestra simulación solo llegue hasta las centésimas de segundo.
Tras cambiar max_step_size a 0.001 y real_time_update_rate a 1000 (necesario tambien) consigo que los tiempos en la simulación
lleguen hasta las milésimas de segundo.INSERTAR IMAGEN 5
Pero tambien he podido comprobar que tras haber cambiado esos parámetros, la frecuencia de publicacion de imagenes 
en el topic /camera/rgb/image_raw es mucho menor, llegando unicamente a 20 hz. INSERTAR IMAGEN 6


Supongo que todos estos son problemas que quizás solo me voy a encontrar haciendo uso de simulaciones, pero desconozco si 20 hz
que supongo que resultará en 20fps es calidad suficiente (me comentaste en un correo que 15 fps es calidad suficiente para 
casi todo). Luego, en cuanto a la precisión de los timestamps, supongo que esto es una pequeña mejora pero no sé si es lo 
suficientemente competente. ¿ Lo cambio entonces para que sea más preciso aunque resulte en una pérdida de frames por segundo ?

5.Es cierto, envié el correo sin pensar mucho en los números. He realizado la simulación de nuevo y he podido comprobar que
estos altos valores se producen solo al principio de ejecutar mi nodo, como en las primeras 20 entradas. Tras ello, obtengo
una diferencia de 0.00, 0.01 o 0.02s entre el momento en el que se captura la imagen y el momento cuando tomo el timestamp
tras el filtro. 
INSERTAR CAPTURA PARTE 5

He realizado esta simulación mientras movía la camará del robot mediante gazebo para evitar que siempre se 
estuviese mandando el mismo frame, la he mantenido durante 2 minutos.
Teniendo la simulación actualizandose cada 1cs, obtengo una media de 0.01042s, es decir,
10.42ms entre la toma de imagen y la toma del timestamp tras el filtro.  

Si cambio el parametro del archivo .world a max_step_size =0.001 para tener actualización de la simulación cada 1ms, 
obtengo  aproximadamente unos 0.00910s de diferencia de media, es decir, 9.1ms entre la toma de imagen y la toma
del timestamp tras el filtro.  

¿Cómo son estos números? ¿Son demasiado grandes aún al tratarse de una simulación que se está utilizando en el mismo ordenador
en el que se capturan los frames?


EL ARCHIVO .WORLD ESTA EN /opt/ros/kinetic/share/turtlebot_gazebo/worlds/playground.world
PROBAR A CAMBIAR EL PARAMETRO DE REAL TIME UPDATE RATE
PROBAR TAMBIEN DESACTIVANDO EL SIM TIME AUNQUE NO PARECE QUE FUNCIONE
https://answers.ros.org/question/35699/gazebo-real-and-simulation-time/
http://gazebosim.org/tutorials?tut=physics_params&cat=physics

14 de abril

19 de abril

<< rostopic bw /camera/rgb/image_raw >> para saber el ancho de banda consumido por un topic

Para cambiar la frecuencia de publicacion de imagenes en la simulacion, hay que ir al archivo: 
/opt/ros/kinect/share/turtlebot_description/urdf/turtlebot_gazebo.urdf.xacro


Segun la documentacion, hay que asignarle un update rate acorde a lo que indique el fabricante del sensor.

Configuracion por defecto: http://gazebosim.org/tutorials?tut=ros_gzplugins#OpenniKinect

Update rate
Number of times per second a new camera image is taken within Gazebo. This is the maximum update rate the sensor 
will attempt during simulation but it could fall behind this target rate if the physics simulation runs faster than 
the sensor generation can keep up.

original en realidad: 20 , 10
Drivers de la camara: https://wiki.ros.org/openni_camera
Guia de parametros: http://gazebosim.org/tutorials/?tut=ros_depth_camera
The updateRate parameter should be set to 0, which will cause the plugin to publish depth information as the same rate as 
the parent SDF sensor. If updateRate is not 0, it will do additional throttling on top of the parent sensor's update rate.

Kinect 1 puede publicar a 5, 15 o 30 fps

22 de abril

Terminado de escribir las hojas que habia empezado sobre datos. Probando conectividad entre servidor y portatil ya que 
es muy mala.

Añadida la variables ROS_IP en el .bashrc para evitar que se formen problemas.

Instalado chrony  y seguidos los pasos del tutorial dentro de http://wiki.ros.org/ROS/NetworkSetup 
para una correcta configuracion de la red. Hay un paso final de chrony que me he saltado porque creo que no hace falta.

<< rosparam set enable_statistics true >> interesante si luego quieres mediante rqt_graph ver las estadisticas  
aunque para mi nodo no sale nada

nada, no hay manera de conseguir que la latencia no sea 300ms 

26 de abril

Añadido la variable a .bashrc de DISPLAY=:0 para poder hacer un roslaunch desde el servidor en el portatil robot y asi no 
tener que movernos hasta el portatil cada vez que queramos hace run roslaunch

Si miro la performance de /camera/rgb/image_raw/compressed se pueden observar muy buenos resultados. Obteniendo la misma hz
que si fuese en local y un delay mucho menor que el conseguido mediante el topic sin comprimir. 

Usar UDP no es factible porque udp no aporta numero de secuencia de paquetes y por lo tanto no lo tendriamos en los mensajes.
Ademas de que creo que en rospy aun no esta implementado.

Mirar si de verdad estoy calculando bien el ms, informarme del uso del topic clock y del use_sim_time. 
A ver, en principio no deberia de tener problemas de que al pillar los timestamps me vayan a dar un retraso desde la 
peticion hasta que me lo devuelve. Teniendo en cuenta la sincronizacion de reloj que deben de tener las maquinas.
Ademas de que si trabajo con la simulacion, no voy a conseguir que me devuelva el timestamp real dispositivos como 
la camara y tal del robot.

La proxima vez tengo que mirar los papers que tengo en google drive y ver que herramientas existen para ver la performance
de los topcis y los nodos. A partir de esas herramientas basarme en la creacion de las mias.
Ademas, debo de aprender a utilizar las imagenes comprimidas.

28 de abril

Instalado rqt. rqt es una herramienta muy potente que contiene muchos plugins utiles.

4 de mayo

5 de mayo

Cuestiones a responder:
Implementar calculo del jitter  DONETE

Implementar ancho de banda consumido
Mirar % de compresion en topic compressed - si es alto, hacer un topic custom con bajada de resolucion de imagen
    Imagenes comprimidas: http://docs.ros.org/en/api/sensor_msgs/html/msg/CompressedImage.html

    Sobre esto, cuando las imagenes estan en RAW se utiliza el encoding BGR8 y las imagenes son de width: 620 x height:480
    Ahora bien, el topic compressed manda imagenes comprimidas de BGR8 a JPEG.

    http://wiki.ros.org/compressed_image_transport#ROS_API API SOBRE LAS IMAGENES COMPRIMIDAS POR OPENNI
    Por defecto jpeg quality al 80% que quiere decir que va a intentar que sea una buena calidad de imagen




Mirar los recursos consumidos por el portatil, bajar los FPS a lso que publica DONE
    Por defecto, arriba 20 abajo 10, por defecto publica a unos 24-25 fps
    Si pongo el update rate a 0.0 de abajo para que suba imagenes en cuanto pueda, y obtengo un hz de 26-27 fps. Esto se debe
    seguramente a uin bottleneck de la gpu que no de para mas.
    Si pongo el update rate de ambos a 0 se publica a unos 100hz pero luego se pierden paquetes
    Si pongo 30 arriba y 0 abajo consigo unos 40 hz
    Si pongo 0 arriba y 30 abajo consigo unos 97 hz
    Si pongo 5.0 arriba y 0 abajo me publica a unos 7hz

    Solo tocar el update rate del sensor, el del plugin hara throttling, osea, podria bajar el update rate pero nunca subirlo.
    Igualmente no me ha funcionado a mi.

    Bajando los FPS a los que publica conseguimos que no haya perdidas de paquetes utilizando las imagenes raw.
    En cuanto a el consumo de recursos del ordenador, no podemos confirmar que se trate de un problema de falta de capacidad
    de computacion por parte de la grafica ya que la grafica que posee, una nvidia 920m no soporta monitorizacion de los procesos
    ni consumo de ellos. Dado que la parte de renderizado de imagenes por parte del simulador es realizado por la grafica,
    no podemos conseguir resultados concluyentes. En realidad si podemos por la memoria consumida en la grafica.

    
Uso de UDP -> creacion del nodo en C++ 
                dado que no esta soportado el uso de udp en nodos creados con python, descartamos esta opcion



Para poner a funcionar la camara del turtlebot quizas tengo que hacer << roslaunch openni_launch openni_launch> o asi

10 de mayo

Como funciona compresion a jpeg
http://wiki.ros.org/image_transport
http://wiki.ros.org/compressed_image_transport
http://wiki.ros.org/theora_image_transport

http://wiki.ros.org/image_transport/Tutorials

http://wiki.ros.org/image_transport/Tutorials/ExaminingImagePublisherSubscriber Para cambiar los settings de la publicacion de
imagenes

<<rosrun rqt_reconfigure rqt_reconfigure>> para cambiar facilmente los parametros de publicacion

sin compresion: ancho de banda consumido por las imagenes =  7.42957414662 MB/s
                            MEAN LATENCY ACHIEVED: 0.273617021277 s
                            tamaño de imagen medio = 0.8828125 MB

cuando jpeg quality a 80 : ancho de banda consumido por las imagenes = 0.2608960522 MB/s
                            MEAN LATENCY ACHIEVED: 0.0214285714284 s
                            tamaño de imagen medio = 0.014205806331547428 MB

cuando jpeg quality a 100 : ancho de banda consumido por las imagenes = 0.869338846287 MB/s
                            MEAN LATENCY ACHIEVED: 0.0175221238939 s
                            tamaño de imagen medio = 0.05570984287422244 MB

cuando jpeg quality a 1: ancho de banda consumido por las imagenes = 0.130256796755 MB/s
                            MEAN LATENCY ACHIEVED: 0.0192342342342 s
                            tamaño de imagen medio = 0.005820772669336817 MB                           
                            
cuando es en png con level a 9 : ancho de banda consumido por las imagenes = 0.415781072328 MB/s 
                                MEAN LATENCY ACHIEVED: 0.165714285714 s 
                                tamaño de imagen medio = 0.0192342342342 MB

cuando es en png con level a 1 : AVERAGE BANDWIDTH CONSUMPTION: 1.42216126601 MB/s
                                MEAN LATENCY ACHIEVED: 0.0321973094169 s
                                tamaño de imagen medio = 0.103034324333316 MB


cuando reducimos resolucion a la mitad: AVERAGE BANDWIDTH CONSUMPTION: 0.859182875018 MB/s
                                MEAN LATENCY ACHIEVED: 2688.27101266 s  ARREGLAR QUEUE
                                tamaño de imagen medio = 0.22250791139240506 MB



Crear mi propio nodo, hacer tambien que el robot se mueva solo.

11 de mayo

Para suscribirse a imagenes
http://wiki.ros.org/cv_bridge/Tutorials/ConvertingBetweenROSImagesAndOpenCVImagesPython


Bastante util:
https://stackoverflow.com/questions/12988151/python-cv2-change-dimension-and-quality

He implementado un filtro que reduce el tamaño de la imagen a la mitad. Arreglar la cola, los tiempos son muy altos.
Desplegar lo de seguir la linea.


12 de mayo

Creo que la cola esta arreglada pero no se por que se pierde el timestamp guardado de la imagen.